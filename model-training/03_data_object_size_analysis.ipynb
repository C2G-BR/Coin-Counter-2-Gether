{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBHcODY5JZmr"
      },
      "source": [
        "# Analysis of object size\n",
        "\n",
        "The purpose of this notebook is to analyze the object size and divide it into three different sections. The clusters can be used later to predict the quality of the predictions for different coin/object sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCI6BGwKJUM4",
        "outputId": "a0c243ea-ee7d-4425-eae7-96d3caf1479f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have forked the package [rafaelpadilla/review_object_detection_metrics](https://github.com/rafaelpadilla/review_object_detection_metrics) and customized it for the purpose of this use case. If you would like to run this notebook, contact us and we will provide you with the customized files. (Note: this script does not need to be executed to train the final model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgYoyZU2KmzY"
      },
      "outputs": [],
      "source": [
        "# Read above\n",
        "!mkdir /usr/lib/python3.7/metrics\n",
        "!cp -R /content/drive/MyDrive/review_object_detection_metrics-main/src /usr/lib/python3.7/metrics/src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8hq9beoKv7A"
      },
      "outputs": [],
      "source": [
        "!pip install PyQt5\n",
        "!pip install -qU torch_snippets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1QvZ-U50SnV"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMcudHJeJsp3"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import glob\n",
        "import torch\n",
        "import time\n",
        "import statistics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from IPython import display\n",
        "from torch_snippets import *\n",
        "from os.path import join\n",
        "from PIL import Image\n",
        "from metrics.src.bounding_box import BoundingBox\n",
        "from metrics.src.evaluators import coco_evaluator, pascal_voc_evaluator\n",
        "from metrics.src.bounding_box import BoundingBox\n",
        "from metrics.src.utils.enumerators import BBFormat, BBType, CoordinatesType, MethodAveragePrecision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPcJfhVJMs5A"
      },
      "outputs": [],
      "source": [
        "width, height = 1000, 1000\n",
        "IMAGE_ROOT = '/content/drive/MyDrive/data/euro-coin-dataset-master'\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZlfdLMTKDAD"
      },
      "outputs": [],
      "source": [
        "rootdir = '/content/drive/MyDrive/data'\n",
        "df_train = pd.read_csv(os.path.join(rootdir, 'train.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOfEYPUGKX89"
      },
      "outputs": [],
      "source": [
        "label2target = {1: 1, 10: 2, 100: 3, 2: 4, 20: 5, 200: 6, 5: 7, 50: 8, 'background': 0}\n",
        "target2label = {0: 'background', 1: 1, 2: 10, 3: 100, 4: 2, 5: 20, 6: 200, 7: 5, 8: 50}\n",
        "num_classes = len(label2target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkp-u-WqMz7W"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(img):\n",
        "  img = torch.tensor(img).permute(2,0,1)\n",
        "  return img.to(device).float()\n",
        "\n",
        "def preprocess_traindata(img, data, target_width, target_height):\n",
        "  current_width, current_height = img.size\n",
        "  data[:,[0,2]] = (data[:,[0,2]] / current_width * target_width).astype(int)\n",
        "  data[:,[1,3]] = (data[:,[1,3]] / current_height * target_height).astype(int)\n",
        "  img = np.array(img.resize((width, height), resample=Image.BILINEAR))/255.\n",
        "  return img, data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bErTvgcbM4N9"
      },
      "outputs": [],
      "source": [
        "class CoinDataset(torch.utils.data.Dataset):\n",
        "  w, h = width, height\n",
        "  def __init__(self, df, image_dir=IMAGE_ROOT, transformer = None, threshold = 0.15):\n",
        "    self.image_dir = image_dir\n",
        "    self.files = glob.glob(IMAGE_ROOT + '/*/*')\n",
        "    self.df = df\n",
        "    self.image_infos = df['filename'].unique()\n",
        "    self.transformer = transformer\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def __getitem__(self, ix):\n",
        "\n",
        "    #filename\tpose\txmin\txmax\tymin\tymax\n",
        "\n",
        "    # load image\n",
        "    image_id = self.image_infos[ix]\n",
        "    img_path = find(image_id, self.files)\n",
        "\n",
        "\n",
        "\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    data = self.df[self.df['filename'] == image_id]\n",
        "    labels = data['pose'].values.tolist()\n",
        "    data = data[['xmin','ymin','xmax','ymax']].values\n",
        "\n",
        "    if self.transformer:\n",
        "      pass # not relevant since for testing data augmentation is not needed\n",
        "\n",
        "    img, data = preprocess_traindata(img, data, self.w, self.h)\n",
        "\n",
        "    boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n",
        "    # torch FRCNN expects ground truths as a dictionary of tensors\n",
        "    target = {}\n",
        "    target['boxes'] = torch.Tensor(boxes).float()\n",
        "    target['labels'] = torch.Tensor([label2target[i] for i in labels]).long()\n",
        "    img = preprocess_image(img)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    return tuple(zip(*batch)) \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_infos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRdgu6yeM5iZ"
      },
      "outputs": [],
      "source": [
        "train_ds = CoinDataset(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCxZVPM7M_MZ"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=batch_size, collate_fn=train_ds.collate_fn, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct-xrlnR0WSU"
      },
      "source": [
        "# Analysis\n",
        "\n",
        "## Create Bounding Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd3h78UlN7C8",
        "outputId": "6de03bbc-1bf0-4465-e247-e515ea9aedc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "bounding_boxes_train = []\n",
        "\n",
        "for ix, (images, targets) in enumerate(train_loader):\n",
        "    #Ground Truth\n",
        "  for i, box in enumerate(targets[0]['boxes']):\n",
        "    cls = target2label[int(targets[0]['labels'][i])]\n",
        "    bb = BoundingBox(\n",
        "      image_name        = str(ix),\n",
        "      class_id          = cls,\n",
        "      coordinates       = list(box),\n",
        "      type_coordinates  = CoordinatesType.ABSOLUTE,\n",
        "      bb_type           = BBType.GROUND_TRUTH,\n",
        "      confidence        = None,\n",
        "      format            = BBFormat.XYX2Y2\n",
        "    )\n",
        "    bounding_boxes_train.append(bb)\n",
        "  if ix % 20 == 0:\n",
        "    p = ix / len(train_loader)\n",
        "    print(p, end = '\\r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2W_yYNzOZAS"
      },
      "outputs": [],
      "source": [
        "areas = []\n",
        "for bbs in bounding_boxes_train:\n",
        "  areas.append(bbs.get_area())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcmxCfC20gQQ"
      },
      "source": [
        "## Cluster object sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ednZQGqdOa37"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(np.array(areas).reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG_z6UJh0kLq"
      },
      "source": [
        "Determine the limit above which an object size changes its cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWSoshjBOdQl"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(min(areas), max(areas), 100000)\n",
        "preds = kmeans.predict(x.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvN3Tf7COfHe",
        "outputId": "896a5668-b5b3-445d-d6b0-6a9ff38e2302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Object boundaries for S and M sized objects [98.50850764 201.88434696]\n"
          ]
        }
      ],
      "source": [
        "changes = np.where(preds[:-1] != preds[1:])[0]\n",
        "print('Object boundaries for S and M sized objects', np.sqrt(x[changes]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Object_Size_Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
